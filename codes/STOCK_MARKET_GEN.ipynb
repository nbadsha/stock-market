{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bXctOoQKTUHk"},"outputs":[],"source":["import math\n","import pandas_datareader as web # version should be 0.10.0\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, LSTM, Dropout\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from openpyxl import load_workbook\n","from datetime import date, datetime\n","import scipy.stats\n","import keras_tuner\n","plt.style.use('fivethirtyeight')\n","\n","\n","\n","scaler = MinMaxScaler(feature_range=(0,1))\n","def plot_stock(df, stock_name):\n","    plt.figure(figsize=(16,8))\n","    plt.title(f\"Clos Price History {stock_name}\")\n","    plt.plot(df['Close'])\n","    plt.xlabel('Date', fontsize=(18))\n","    plt.ylabel('Clos Price', fontsize=(18))\n","    plt.show()\n","    \n","\n","def scale_data(dataset):    \n","    scaled_data = scaler.fit_transform(dataset)\n","    return scaled_data\n","\n","def filter_data(df,column):\n","    # create a new dataframe with only close column\n","    data = df.filter([column])\n","    return data\n","\n","\n","\n","def train_test_split(scaled_data, training_data_len, window_size):\n","    # <<<<<<<<<<<<<<<<<<<<<<<<<<Training Data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","    # create the scaled training dataset\n","    train_data = scaled_data[0:training_data_len,:]\n","\n","    # split the data into x_train and y_train data sets\n","    x_train = []\n","    y_train = []\n","    p_n = window_size\n","    for i in range(p_n, len(train_data)):\n","        x_train.append(train_data[i-p_n:i,0])\n","        y_train.append(train_data[i, 0])\n","            \n","    # convert the x_train and y_train to numpy arrays\n","    x_train, y_train = np.array(x_train), np.array(y_train)\n","\n","    # reshape the data\n","    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n","    # <<<<<<<<<<<<<<<<<<<<<<<<<<Training Data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","    \n","    # <<<<<<<<<<<<<<<<<<<<<<<<<<Test Data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","    # create the testting data set\n","    # a new array containing scaled values from index 1543 to 2003\n","    test_data = scaled_data[training_data_len - window_size: , :]\n","    # creat x_test and y_test\n","    x_test = []\n","    y_test = scaled_data[training_data_len:,:]\n","    for i in range(window_size, len(scaled_data) - (training_data_len - window_size)):\n","        x_test.append(test_data[i-window_size:i,0])\n","        \n","    # convert the data to numpy array\n","    x_test = np.array(x_test)\n","\n","    # reshape the data\n","    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n","    # <<<<<<<<<<<<<<<<<<<<<<<<<<Test Data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","    \n","    return x_train, y_train, x_test, y_test\n","\n","\n","\n","def get_training_data_len(df, training_size):\n","    # get the number of rows to train the model on\n","    return math.ceil(len(df) * training_size)\n","\n","\n","def build_train_model(units, activation,lr, drop_out):\n","    # LSTM MODEL\n","    input_timesteps= 20 - 1\n","    model = Sequential()\n","    # model.add(LSTM(units=units, input_shape=(input_timesteps, 1), return_sequences = True))\n","    model.add(LSTM(units=units, return_sequences = True))\n","    if drop_out:\n","        model.add(Dropout(rate=0.25))\n","    model.add(LSTM(units=units,return_sequences = True))\n","    model.add(LSTM(units=units,return_sequences =False))\n","    if drop_out:\n","        model.add(Dropout(rate=0.25))\n","    model.add(Dense(units=units, activation=activation))  \n","    model.add(Dense(1, activation='linear'))\n","    model.compile(loss='mean_squared_error',optimizer=keras.optimizers.Adam(learning_rate=lr))\n","    # Fit the model\n","    # model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size)\n","    \n","    return model\n","    # model.predict(x_test)\n","    \n","    \n","def build_model(hp):\n","    units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n","    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n","    dropout = hp.Boolean(\"dropout\")\n","    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n","    # call existing model-building code with the hyperparameter values.\n","    model = build_train_model(\n","        units=units, activation=activation, drop_out=dropout, lr=lr\n","    )\n","    return model\n","\n","def model_predict(model, x_test):\n","  # get the model's predicted price values\n","  preds = model.predict(x_test)\n","  return preds\n","\n","def run_model_iteration(df, hyper_params):\n","    # filter data with only close column\n","    data = filter_data(df, 'Close')    \n","    # scale data with Min Max Scaler\n","    scaled_dataset = scale_data(data.values)\n","    run_data = []\n","    models = []\n","    for index, hyper_param in hyper_params.iterrows():\n","        trainig_data_size =  hyper_param[0]\n","        hyper_param = hyper_param[1:].astype(int)\n","        training_data_len = get_training_data_len(data, trainig_data_size)\n","    \n","        # split data into train test\n","        x_train, y_train, x_test, y_test = \\\n","            train_test_split(scaled_dataset, training_data_len, hyper_param['window_size'])\n","        \n","        # model build and train\n","        # model = model_build_train(x_train = x_train,\\\n","        #                           y_train = y_train,\n","        #                           lstm_input_layer = hyper_param['lstm_input_layer'],\n","        #                           lstm_middle_layer = hyper_param['lstm_middle_layer'],\n","        #                           dense_layer = hyper_param['dense_layer'],\n","        #                           epochs = hyper_param['epochs'])\n","        # models.append(model)\n","        tuner = keras_tuner.RandomSearch(\n","            hypermodel=build_model,\n","            objective=[\"val_accuracy\"],\n","            max_trials=1,\n","            executions_per_trial=1,\n","            overwrite=True,\n","            directory=\"model_dir\",\n","            project_name=\"stock\",\n","        )\n","        \n","        tuner.search(x_train, y_train, epochs=1, validation_data=(x_test, y_test))\n","        \n","        models = tuner.get_best_models(num_models=2)\n","        model = models[0]\n","        y_preds = scaler.inverse_transform(model_predict(model, x_test))\n","        \n","        y_test = scaler.inverse_transform(y_test)\n","        \n","        rmse = get_rmse(y_preds, y_test)\n","        accuracy = get_accuracy(y_test, y_preds)\n","        \n","        train_data, validation_data = \\\n","            consolidate_train_validation_data(data, y_preds, training_data_len)\n","        # plot_after_pred(train_data, validation_data)\n","        \n","        # val_data = validation_data.copy()\n","        run_info = {\n","            'Run'               : index,\n","            'trainig_data_len'  : trainig_data_size,\n","            'window_size'       : hyper_param['window_size'],\n","            # 'lstm_input_layer'  : hyper_param['lstm_input_layer'],\n","            # 'lstm_middle_layer' : hyper_param['lstm_middle_layer'],\n","            # 'dense_layer'       : hyper_param['dense_layer'],\n","            # 'epochs'            : hyper_param['epochs'],\n","            'RMSE'              : rmse,\n","            'Accuracy'          : accuracy\n","        }\n","        run_data.append(run_info)\n","        print(model.summary())\n","        print(run_info)\n","    return run_data, models\n","\n","def get_accuracy(real, predict):\n","    real = np.array(real) + 1\n","    predict = np.array(predict) + 1\n","    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n","    return percentage * 100\n","\n","def get_rmse(y_preds, y_test):\n","    # get root mean squared error (RMSE)\n","    rmse = np.sqrt(np.mean(y_preds - y_test)**2)\n","    return rmse\n","\n","def df_index_alter(df):\n","    df = df.sort_values(by=['Date'],ascending=True)\n","    # setting the index as date\n","    df.index = df['Date']\n","    # drop date column as it is now index column\n","    df = df.drop(['Date'],axis=1)\n","    return df\n","\n","def consolidate_train_validation_data(df, preds, training_data_len):\n","    # plot the data\n","    train = df[:training_data_len].copy()\n","    valid = df[training_data_len:].copy()\n","    valid['Predictions'] = preds\n","    return train, valid\n","\n","def plot_after_pred(train, valid, title, stock_name):\n","    # visulize the data\n","    plt.figure(figsize=(16,8))\n","    plt.title(title)\n","    plt.xlabel('Date', fontsize=18)\n","    plt.ylabel(f'Closr Price {stock_name}', fontsize=18)\n","    plt.plot(train['Close'])\n","    plt.plot(valid[['Close', 'Predictions']])\n","    plt.legend(['Train', 'Actual', 'Predictions'], loc='lower right')\n","    plt.show()\n","    return\n","\n","def forecast(model, window_size, scaled_dataset, days):    \n","    last_window_size = len(scaled_dataset)-window_size\n","    last_winow_df = scaled_dataset[last_window_size:]\n","    last_winow_df = last_winow_df.reshape((1, window_size, 1))\n","    \n","    future_days = days\n","    future_preds = []\n","    temp_data = []\n","    for day in range(future_days):\n","        if day == 0:        \n","            y_hat = model.predict(last_winow_df)\n","            y_hat_ = scaler.inverse_transform(y_hat)\n","            future_preds.append(y_hat_[0][0])\n","            temp_data = list(last_winow_df.flatten())\n","            temp_data.append(y_hat[0][0])\n","            temp_data = temp_data[1:]\n","        else:\n","            temp_data = np.array(temp_data).reshape((1, window_size, 1))\n","            y_hat = model.predict(temp_data)\n","            y_hat_ = scaler.inverse_transform(y_hat)\n","            future_preds.append(y_hat_[0][0])\n","            temp_data = list(temp_data.flatten())\n","            temp_data.append(y_hat[0][0])\n","            temp_data = temp_data[1:]\n","    return future_preds\n","\n","\n","def mean_confidence_interval(data, confidence):\n","    a = 1.0 * np.array(data)\n","    n = len(a)\n","    m, se = np.mean(a), scipy.stats.sem(a)\n","    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","    return m, m-h, m+h\n","\n","def check_forecast(forecasted_data, upper_bound, lower_bound):\n","    forecast_df = pd.DataFrame({'Forecast':forecasted_data})    \n","    forecast_df['upper_bound'] = upper_bound\n","    forecast_df['lower_bound'] = lower_bound\n","    forecast_df['is_in_between'] = forecast_df['Forecast'].between(forecast_df['lower_bound'],\\\n","                                                                   forecast_df['upper_bound'],  inclusive='both')\n","    return forecast_df\n","\n","def model_evalution_forecasting(model_path, df_path, window_size, training_data_size, future_days, stock_name, ci):\n","    # get NASDAQ dataset\n","    df = pd.read_excel(df_path)\n","    \n","    # alter index to date\n","    df = df_index_alter(df)\n","    \n","    # plot the whole data\n","    plot_stock(df, stock_name)    \n","    \n","    model = load_model(model_path)\n","    # filter data with only close column\n","    data = filter_data(df, 'Close')\n","    \n","    # scale data with Min Max Scaler\n","    scaled_dataset = scale_data(data.values)\n","    \n","    # get training data length\n","    training_data_len = get_training_data_len(data, training_data_size)\n","    # split data into train test\n","    x_train, y_train, x_test, y_test =\\\n","        train_test_split(scaled_dataset, training_data_len, window_size)\n","    t1 = scaler.inverse_transform(model.predict(x_test))\n","    train, valid = consolidate_train_validation_data(df[['Close']], t1, training_data_len)\n","    future_close = forecast(model, window_size, scaled_dataset, future_days)\n","    \n","    d = pd.DataFrame(future_close,columns=['Predictions'])\n","    d.index = pd.date_range(date(2022, 11, 21), date(2022, 11, 25), freq='D')\n","    valid1 = valid.append(d)\n","    plot_after_pred(train, valid1, stock_name, stock_name)    \n","    mean, lower_bound, upper_bound = mean_confidence_interval(future_close,confidence=ci)\n","    forecast_df = check_forecast(future_close, upper_bound, lower_bound)\n","    forecast_df.index = pd.date_range(date(2022, 11, 21), date(2022, 11, 25), freq='D')\n","    \n","    return df, train, valid, valid1, forecast_df"]}]}